{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Matrix(*a):\n",
    "    if len(a)==1 and isinstance(a[0], np.ndarray):\n",
    "        a = a[0]\n",
    "    return np.array([[float(x) for x in r] for r in a])\n",
    "\n",
    "def Vector(*a):\n",
    "    if len(a)==1 and isinstance(a[0], np.ndarray):\n",
    "        a = a[0]\n",
    "    return np.array([float(x) for x in a]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Black magic\n",
    "from IPython.display import Latex, SVG, display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "def ndarray_to_latex(arr): \n",
    "    if len(arr.shape)==1: \n",
    "        arr=arr.reshape(1,-1)\n",
    "    if len(arr.shape) != 2:\n",
    "        return None\n",
    "    str_arr = np.vectorize(\"{:.3f}\".format)(arr)\n",
    "    return r'\\begin{{pmatrix}}{}\\end{{pmatrix}}'.format(r'\\\\ '.join(map('&'.join, str_arr))) \n",
    "sh = InteractiveShell.instance()\n",
    "sh.display_formatter.formatters['text/latex'].type_printers[np.ndarray]=ndarray_to_latex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning for classification\n",
    "給一堆 x, 我們想用 x 來預測他的分類\n",
    "\n",
    "### One hot encoding\n",
    "如果我們有三類東西， 我們可以來編碼這三個類別\n",
    "* $(1,0,0)$\n",
    "* $(0,1,0)$\n",
    "* $(0,0,1)$\n",
    "\n",
    "### 問題\n",
    "* 為什麼不直接用 1,2,3 這樣的編碼呢？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression 的模型是這樣的\n",
    "我們的輸入 $x=\\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} $ 是一個向量，我們看成 column vector 好了\n",
    "\n",
    "而 Weight: $W = \\begin{pmatrix} W_0 \\\\ W_1 \\\\ W_2 \\end{pmatrix} =  \n",
    "\\begin{pmatrix} W_{0,0} & W_{0,1} &  W_{0,2} & W_{0,3}\\\\ \n",
    " W_{1,0} & W_{1,1} &  W_{1,2} & W_{1,3} \\\\ \n",
    " W_{2,0} & W_{2,1} &  W_{2,2} & W_{2,3} \\end{pmatrix} $\n",
    " \n",
    " Bias: $b=\\begin{pmatrix} b_0 \\\\ b_1 \\\\ b_2 \\end{pmatrix} $ \n",
    "\n",
    "\n",
    "我們先計算\"線性輸出\"  $ c = \\begin{pmatrix} c_0 \\\\ c_1 \\\\ c_2 \\end{pmatrix} =  Wx+b =\n",
    "\\begin{pmatrix} W_0 x + b_0 \\\\ W_1 x + b_1 \\\\ W_2 x + b_2 \\end{pmatrix}   $， 然後再取 $exp$ (逐項取)。 最後得到一個向量。\n",
    " \n",
    " $d = \\begin{pmatrix} d_0 \\\\ d_1 \\\\ d_2 \\end{pmatrix} = e^{W x + b} = \\begin{pmatrix} e^{c_0} \\\\ e^{c_1} \\\\ e^{c_2} \\end{pmatrix}$\n",
    "\n",
    "\n",
    "將這些數值除以他們的總和。\n",
    "我們希望出來的數字會符合這張圖片是這個數字的條件機率。\n",
    "\n",
    "###  $q(i) = Predict(Y=i|x, W, b)  = \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}} = \\frac {d_i} {\\sum_j d_j}$\n",
    "\n",
    "### 問題\n",
    "* 為什麼要用 $exp$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先隨便算一個 $\\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$ 的網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000&2.000\\\\ 3.000&4.000\\\\ 5.000&6.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.,  2.],\n",
       "       [ 3.,  4.],\n",
       "       [ 5.,  6.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight\n",
    "W = Matrix([1,2],[3,4], [5,6])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ 0.000\\\\ -1.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 0.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bias\n",
    "b = Vector(1,0,-1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}2.000\\\\ -1.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 2.],\n",
       "       [-1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 輸入\n",
    "x = Vector(2,-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 請計算 最後的 $\\Pr$ \n",
    "Hint: `np.exp` 可以算 $exp$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.000\\\\ 2.000\\\\ 3.000\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wx+b\n",
    "c = W @ x + b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}2.718\\\\ 7.389\\\\ 20.086\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[  2.71828183],\n",
       "       [  7.3890561 ],\n",
       "       [ 20.08553692]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exp(Wx+b)\n",
    "d = np.exp(c)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.090\\\\ 0.245\\\\ 0.665\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.09003057],\n",
       "       [ 0.24472847],\n",
       "       [ 0.66524096]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax\n",
    "Q = d/d.sum()\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習\n",
    "設計一個網路:\n",
    "* 輸入是二進位 0 ~ 15\n",
    "* 輸出依照對於 4 的餘數分成四類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0 predict: 0 ground truth: 0\n",
      "i= 1 predict: 1 ground truth: 1\n",
      "i= 2 predict: 2 ground truth: 2\n",
      "i= 3 predict: 3 ground truth: 3\n",
      "i= 4 predict: 0 ground truth: 0\n",
      "i= 5 predict: 1 ground truth: 1\n",
      "i= 6 predict: 2 ground truth: 2\n",
      "i= 7 predict: 3 ground truth: 3\n",
      "i= 8 predict: 0 ground truth: 0\n",
      "i= 9 predict: 1 ground truth: 1\n",
      "i= 10 predict: 2 ground truth: 2\n",
      "i= 11 predict: 3 ground truth: 3\n",
      "i= 12 predict: 0 ground truth: 0\n",
      "i= 13 predict: 1 ground truth: 1\n",
      "i= 14 predict: 2 ground truth: 2\n",
      "i= 15 predict: 3 ground truth: 3\n"
     ]
    }
   ],
   "source": [
    "# 參考答案\n",
    "W = Matrix([-1,-1,0,0], [1,-1,0,0], [-1,1,0,0], [1,1,0,0])\n",
    "b = Vector(0,0,0,0)\n",
    "for i in range(16):\n",
    "    x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "    r = W @ x + b\n",
    "    print(\"i=\", i, \"predict:\", r.argmax(), \"ground truth:\", i%4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習\n",
    "設計一個網路:\n",
    "* 輸入是二進位 0 ~ 15\n",
    "* 輸出依照對於 3 的餘數分成三類\n",
    "\n",
    "Hint: 不用全部正確，用猜的，但正確率要比亂猜高。可以利用統計的結果猜猜看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0 predict: 0 ground truth: 0\n",
      "i= 1 predict: 1 ground truth: 1\n",
      "i= 2 predict: 2 ground truth: 2\n",
      "i= 3 predict: 0 ground truth: 0\n",
      "i= 4 predict: 1 ground truth: 1\n",
      "i= 5 predict: 1 ground truth: 2\n",
      "i= 6 predict: 0 ground truth: 0\n",
      "i= 7 predict: 1 ground truth: 1\n",
      "i= 8 predict: 2 ground truth: 2\n",
      "i= 9 predict: 0 ground truth: 0\n",
      "i= 10 predict: 2 ground truth: 1\n",
      "i= 11 predict: 2 ground truth: 2\n",
      "i= 12 predict: 0 ground truth: 0\n",
      "i= 13 predict: 1 ground truth: 1\n",
      "i= 14 predict: 2 ground truth: 2\n",
      "i= 15 predict: 0 ground truth: 0\n"
     ]
    }
   ],
   "source": [
    "# 參考答案\n",
    "W = Matrix([0,0,0,0], [1,-1,1,-1], [-1,1,-1,1])\n",
    "b = Vector(0.1,0,0)\n",
    "for i in range(16):\n",
    "    x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "    r = W @ x + b\n",
    "    print(\"i=\", i, \"predict:\", r.argmax(), \"ground truth:\", i%3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 誤差函數\n",
    "為了要評斷我們的預測的品質，要設計一個評斷誤差的方式\n",
    "\n",
    "假設輸入值 $x$ 對應到的真實類別是 $y$, 那我們定義誤差函數\n",
    "\n",
    "## $ loss = -\\log(q(y))=- \\log(Predict(Y=y|x, W,b)) $\n",
    "\n",
    "\n",
    "其實比較一般但比較複雜一點的寫法是\n",
    "\n",
    "## $ loss = - \\sum_i p(i)\\log(q(i))  $\n",
    "\n",
    "其中 $i$ 是所有類別， 而 $ p(i) = \\Pr(Y=i|x) $ 是真實發生的機率\n",
    "\n",
    "但我們目前 $x$ 對應到的真實類別是 $y$， 所以直接 $p(y)=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 想辦法改進。 \n",
    "我們用一種被稱作是 gradient descent 的方式來改善我們的誤差。\n",
    "\n",
    "因為我們知道 gradient 是讓函數上升最快的方向。所以我們如果朝 gradient 的反方向走一點點（也就是下降最快的方向），那麼得到的函數值應該會小一點。\n",
    "\n",
    "記得我們的變數是 $W$ 和 $b$ (裡面有一堆 W_i,j b_i 這些變數)，所以我們要把 $loss$ 對 $W$ 和 $b$ 裡面的每一個參數來偏微分。\n",
    "\n",
    "還好這個偏微分是可以用手算出他的形式，而最後偏微分的式子也不會很複雜。\n",
    "\n",
    "$loss$ 展開後可以寫成\n",
    "## $loss = -\\log(q(y)) = \\log(\\sum_j d_j) - d_i \\\\\n",
    " = \\log(\\sum_j e^{W_j x + b_j}) - W_i x - b_i$\n",
    "\n",
    "注意 $d_j = e^{W_j x + b_j}$ 只有變數 $b_j, W_j$ \n",
    "\n",
    " 對 $k \\neq i$ 時, $loss$ 對 $b_k$ 的偏微分是 \n",
    " $$ \\frac{e^{W_k x + b_k}}{\\sum_j e^{W_j x + b_j}} = q(k)$$\n",
    "對 $k = i$ 時, $loss$ 對 $b_k$ 的偏微分是 \n",
    "$$ q(k) - 1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "對 $W$ 的偏微分也不難\n",
    "\n",
    " 對 $k \\neq i$ 時, $loss$ 對 $W_{k,t}$ 的偏微分是 \n",
    " $$ \\frac{e^{W_k x + b_k} W_{k,t} x_t}{\\sum_j e^{W_j x + b_j}} = q(k) x_t$$\n",
    "對 $k = i$ 時, $loss$ 對 $W_{k,t}$ 的偏微分是 \n",
    "$$ q(k) x_t - x_t$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = Matrix(np.random.normal(size=(3,4)))\n",
    "b = Vector(np.random.normal(size=(3,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}-0.023&0.677&0.240&0.790\\\\ 1.617&-0.274&-2.339&0.916\\\\ -1.037&-0.506&-0.046&-0.228\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[-0.02316323,  0.67736647,  0.24009028,  0.78964975],\n",
       "       [ 1.61717265, -0.27397296, -2.33857055,  0.91615159],\n",
       "       [-1.03655084, -0.50555928, -0.04595218, -0.22805642]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}1.678\\\\ 0.499\\\\ -0.820\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 1.67812739],\n",
       "       [ 0.49914493],\n",
       "       [-0.8202958 ]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.983\\\\ 0.010\\\\ 0.007\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.98321785],\n",
       "       [ 0.01005781],\n",
       "       [ 0.00672434]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 14\n",
    "x = Vector(i%2, (i>>1)%2, (i>>2)%2, (i>>3)%2)\n",
    "y = i%3\n",
    "d = np.exp(W @ x + b)\n",
    "q = d/d.sum()\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}5.002\\end{pmatrix}"
      ],
      "text/plain": [
       "array([ 5.00202213])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -np.log(q[y])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.983\\\\ 0.010\\\\ -0.993\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.98321785],\n",
       "       [ 0.01005781],\n",
       "       [-0.99327566]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_b = q.copy()\n",
    "grad_b[y] -= 1\n",
    "grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.000&0.983&0.983&0.983\\\\ 0.000&0.010&0.010&0.010\\\\ 0.000&-0.993&-0.993&-0.993\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.        ,  0.98321785,  0.98321785,  0.98321785],\n",
       "       [ 0.        ,  0.01005781,  0.01005781,  0.01005781],\n",
       "       [ 0.        , -0.99327566, -0.99327566, -0.99327566]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_W =   q @ x.T\n",
    "grad_W[y] -= x.ravel()\n",
    "grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W -= 0.5 * grad_W\n",
    "b -= 0.5 * grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.529\\\\ 0.056\\\\ 0.415\\end{pmatrix}"
      ],
      "text/plain": [
       "array([[ 0.52877451],\n",
       "       [ 0.05590564],\n",
       "       [ 0.41531985]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.exp(W @ x + b)\n",
    "q = d/d.sum()\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{pmatrix}0.879\\end{pmatrix}"
      ],
      "text/plain": [
       "array([ 0.87870633])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -np.log(q[y])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
